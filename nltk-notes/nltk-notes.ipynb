{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on learning NLTK\n",
    "\n",
    "This notebook is a collection of notes and codes developed during my studies of the NLTK library.\n",
    "\n",
    "## Index\n",
    "* [Setting Up the Environment](#setting-up-the-environment)\n",
    "* [Language Processing and Python](#language-processing-and-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "## Setting Up the Environment\n",
    "NLTK stands for **Natural Language Toolkit** and it is the most used Python library used to work with human language. More information about it can be found on the website [www.nltk.org](www.nltk.org).\n",
    "\n",
    "At the moment of writing, the following is installed system-wide:\n",
    "\n",
    "```\n",
    "C:\\nltk>python\n",
    "Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:06:47) [MSC v.1914 32 bit (Intel)] on win32\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    "\n",
    "C:\\notebooks>pip list\n",
    "Package            Version\n",
    "------------------ -------\n",
    "backcall           0.1.0\n",
    "bleach             3.0.2\n",
    "colorama           0.4.0\n",
    "decorator          4.3.0\n",
    "defusedxml         0.5.0\n",
    "entrypoints        0.2.3\n",
    "ipykernel          5.1.0\n",
    "ipython            7.1.1\n",
    "ipython-genutils   0.2.0\n",
    "ipywidgets         7.4.2\n",
    "jedi               0.13.1\n",
    "Jinja2             2.10\n",
    "jsonschema         2.6.0\n",
    "jupyter            1.0.0\n",
    "jupyter-client     5.2.3\n",
    "jupyter-console    6.0.0\n",
    "jupyter-core       4.4.0\n",
    "MarkupSafe         1.0\n",
    "mistune            0.8.4\n",
    "nbconvert          5.4.0\n",
    "nbformat           4.4.0\n",
    "notebook           5.7.0\n",
    "pandocfilters      1.4.2\n",
    "parso              0.3.1\n",
    "pickleshare        0.7.5\n",
    "pip                18.1\n",
    "prometheus-client  0.4.2\n",
    "prompt-toolkit     2.0.7\n",
    "Pygments           2.2.0\n",
    "python-dateutil    2.7.5\n",
    "pywinpty           0.5.4\n",
    "pyzmq              17.1.2\n",
    "qtconsole          4.4.2\n",
    "Send2Trash         1.5.0\n",
    "setuptools         40.5.0\n",
    "six                1.11.0\n",
    "terminado          0.8.1\n",
    "testpath           0.4.2\n",
    "tornado            5.1.1\n",
    "traitlets          4.3.2\n",
    "virtualenv         16.0.0\n",
    "wcwidth            0.1.7\n",
    "webencodings       0.5.1\n",
    "widgetsnbextension 3.4.2\n",
    "```\n",
    "\n",
    "The first thing to do in order to work with NLTK is to install it. Detailed instructions are available on the [Installing NLTK](http://www.nltk.org/install.html) page. In this section, I will detail what I installed on my Windows system. Should I ever install it on another system, I will integrate it.\n",
    "\n",
    "\n",
    "### Install Numpy\n",
    "[NumPy](http://www.numpy.org/) is the fundamental package for scientific computing with Python. Install it on your system with the following command\n",
    "\n",
    "```\n",
    "C:\\notebooks>pip install numpy\n",
    "Collecting numpy\n",
    "  Downloading https://files.pythonhosted.org/packages/42/5a/eaf3de1cd47a5a6baca4\n",
    "1215fba0528ee277259604a50229190abf0a6dd2/numpy-1.15.4-cp37-none-win32.whl (9.9MB\n",
    ")\n",
    "    100% |████████████████████████████████| 9.9MB 2.1MB/s\n",
    "Installing collected packages: numpy\n",
    "Successfully installed numpy-1.15.4\n",
    "```\n",
    "\n",
    "### Install NLTK\n",
    "In order to install NLTK libraries, run the following command:\n",
    "\n",
    "```\n",
    "(nltk) C:\\GitHub\\nltk>pip install nltk\n",
    "Collecting nltk\n",
    "Collecting six (from nltk)\n",
    "  Using cached https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bf\n",
    "a78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl\n",
    "Installing collected packages: six, nltk\n",
    "Successfully installed nltk-3.3 six-1.11.0\n",
    "```\n",
    "\n",
    "With this, we are ready to start working with NLTK, and we'll start by following the [Natural Language Processing with Python](http://www.nltk.org/book/) book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "## Language Processing and Python\n",
    "In this notebook, I will focus exclusively on the NLTK argument, skipping all the Python related parts of the book. **This is NOT a replacement for reading the book and walk the path yourself!**\n",
    "\n",
    "Let's start by downloading and install the data required for the book, by executing the following code that will install all the data into the local directory `.\\nltk-data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package brown to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package chat80 to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package cmudict to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package conll2000 to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to .\\nltk-\n",
      "[nltk_data]    |     data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package genesis to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package names to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package ppattach to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package reuters to .\\nltk-data...\n",
      "[nltk_data]    | Downloading package senseval to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package state_union to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package swadesh to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package timit to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package treebank to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package toolbox to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package udhr to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package webtext to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to .\\nltk-\n",
      "[nltk_data]    |     data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to .\\nltk-\n",
      "[nltk_data]    |     data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package punkt to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package city_database to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package tagsets to .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to .\\nltk-data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     .\\nltk-data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('book',download_dir='.\\\\nltk-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load the content of the `book`'s data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
